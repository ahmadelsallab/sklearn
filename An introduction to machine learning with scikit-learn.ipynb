{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample datasets:\n",
    "    scikit-learn comes with a few standard datasets, for instance the iris and digits datasets for classification and the boston house prices dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset contains:\n",
    "    - data: [n_samples, n_features]\n",
    "    - target: [n_samples,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digits.data)\n",
    "print(digits.data.shape)#(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You could plot from data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.reshape(digits.data[0], [8, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digits.target)\n",
    "print(digits.target.shape)\n",
    "print(digits.images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = iris\n",
    "#print(dataset.data)\n",
    "print(dataset.data.shape)#(n_samples, n_features)\n",
    "#print(dataset.target)\n",
    "print(dataset.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = boston\n",
    "#print(dataset.data)\n",
    "print(dataset.data.shape)#(n_samples, n_features)\n",
    "#print(dataset.target)\n",
    "print(dataset.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load and prepare data\n",
    "- Model selection\n",
    "- Model fitting\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model = SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In sklearn, we have:\n",
    "    - SVC --> SVM Classifier\n",
    "    - SVR --> SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit ao all data except the last example--> keep for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(digits.data[:-1], digits.target[:-1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(digits.data[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(digits.data[-1:], [8, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model persistence (save/load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using python only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "s = pickle.dumps(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_loaded = pickle.loads(s)\n",
    "print(model_loaded.predict(digits.data[-1:]))\n",
    "plt.imshow(np.reshape(digits.data[-1:], [8, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pickle has some security and maintainability issues. Please refer to section Model persistence (http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence) for more detailed information about model persistence with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the specific case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump & joblib.load), which is more efficient on big data, but can only pickle to the disk and not to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'model_joblib.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_loaded_joblib = joblib.load('model_joblib.pkl') \n",
    "print(model_loaded_joblib.predict(digits.data[-1:]))\n",
    "plt.imshow(np.reshape(digits.data[-1:], [8, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn estimators follow certain rules to make their behavior more predictive.\n",
    "Check: http://scikit-learn.org/stable/tutorial/basic/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unless otherwise specified, input will be cast to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import random_projection\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.rand(10, 2000)\n",
    "X = np.array(X, dtype='float32')\n",
    "print('Input type is: ', X.dtype)\n",
    "\n",
    "\n",
    "transformer = random_projection.GaussianRandomProjection()\n",
    "X_new = transformer.fit_transform(X)\n",
    "print('After sklearn fit_transform, the input is cast to float64', X_new.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above rule is applicable only to regression targets, not classification. Classification targets are mentained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "iris = datasets.load_iris()\n",
    "clf = SVC()\n",
    "clf.fit(iris.data, iris.target)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list(clf.predict(iris.data[:3]))\n",
    "\n",
    "\n",
    "clf.fit(iris.data, iris.target_names[iris.target])  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list(clf.predict(iris.data[:3]))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-fitting hyper params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters of an estimator can be updated after it has been constructed via the sklearn.pipeline.Pipeline.set_params method. Calling fit() more than once will overwrite what was learned by any previous fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.rand(100, 10)\n",
    "y = rng.binomial(1, 0.5, 100)\n",
    "X_test = rng.rand(5, 10)\n",
    "\n",
    "\n",
    "clf = SVC()\n",
    "print('Initial SVC model constructed')\n",
    "\n",
    "print('Override the kernel to linear')\n",
    "clf.set_params(kernel='linear').fit(X, y)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(clf.predict(X_test))\n",
    "\n",
    "\n",
    "print('Override the kernel to rbf')\n",
    "clf.set_params(kernel='rbf').fit(X, y)  \n",
    "\n",
    "\n",
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target labels encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between ordinal and nominal\n",
    "http://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/\n",
    "\n",
    "    \n",
    "## Nominal: \n",
    "\n",
    "Nominal scales are used for labeling variables, without any quantitative value.  “Nominal” scales could simply be called “labels.”  Here are some examples, below.  Notice that all of these scales are mutually exclusive (no overlap) and none of them have any numerical significance.  A good way to remember all of this is that “nominal” sounds a lot like “name” and nominal scales are kind of like “names” or labels.\n",
    "    \n",
    "    \n",
    "## Oridnal: \n",
    "\n",
    "With ordinal scales, it is the order of the values is what’s important and significant, but the differences between each one is not really known.  Take a look at the example below.  \n",
    "\n",
    "In each case, we know that a #4 is better than a #3 or #2, but we don’t know–and cannot quantify–how much better it is.  \n",
    "\n",
    "For example, is the difference between “OK” and “Unhappy” the same as the difference between “Very Happy” and “Happy?”  We can’t say.\n",
    "\n",
    "Ordinal scales are typically measures of non-numeric concepts like satisfaction, happiness, discomfort, etc.\n",
    "\n",
    "“Ordinal” is easy to remember because is sounds like “order” and that’s the key to remember with “ordinal scales”–it is the order that matters, but that’s all you really get from these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorial case (LableEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input is an array of strings, and we want to fit NOMINAL or ORDINAL (higher value is higher importance), we use LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "print('Raw labels are: ', [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "print('The oridnal of [\"tokyo\", \"tokyo\", \"paris\"] is: ', le.transform([\"tokyo\", \"tokyo\", \"paris\"]) )\n",
    "\n",
    "print('The inverse transofrm into strings of [2 2 1] is: ', list(le.inverse_transform([2, 2, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may even have input categories as numbers, not as strings. \n",
    "\n",
    "We may want to give them ascending order \"RANKs\" (0,1,2,3...etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Raw labels are: ', [1, 2, 2, 6])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([1, 2, 2, 6])\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "print('The oridnal of [1, 1, 2, 6] is: ', le.transform([1, 1, 2, 6]) )\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform([0, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass case on ordinal data (LabelBinarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classifiers are binary, only produce 1 or 0. To transform it into multi-class case (n_classes), a common way is to use one-vs-all.\n",
    "\n",
    "In this case, you n_classes classifiers, each is trained on 0/1 task. If the input instance belongs to a class, then its desired output is 1 and all others are zeros.\n",
    "\n",
    "This mandates to encode the targets in a special way, also known as one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "print('Normally, the target is formatted as: ', y)\n",
    "\n",
    "y = LabelBinarizer().fit_transform(y)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The one-hot-encoding is mandatory for the NN softmax for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encoder (OHE) on categorial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we transform to ordinal using LabelEncoder then use LabelBinarizer to get the OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_raw = [1, 2, 2, 6]\n",
    "print('Raw labels are: ', y_raw)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_raw)\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "y_ordinal = le.transform(y_raw)\n",
    "print('The oridnal of [1, 1, 2, 6] is: ', y_ordinal )\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform(y_ordinal))\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "y_OHE = LabelBinarizer().fit_transform(y_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', y_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-labels vs. Multi-Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we may need to assign more than one label for an instance/example.\n",
    "\n",
    "This is encountered in case of multi-task learning.\n",
    "\n",
    "Example: https://arxiv.org/abs/1612.07695\n",
    "        \n",
    "In this case, we can encode more than one position as 1 at a time.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0]\n",
      " [1 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [1 0 1 1 0]\n",
      " [0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]\n",
    "y = MultiLabelBinarizer().fit_transform(y)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One vs ALL classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After formatting the target variables, how to fit a one vs. ALL classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "\n",
    "y = LabelBinarizer().fit_transform(y)\n",
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneVsRestClassifier can also work on categorial y directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "\n",
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding of the INPUT features (not targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset (Design matrix X [n_samples, n_features].\n",
    "\n",
    "And we have ALL or SOME of the features as categorial/ordinal.\n",
    "\n",
    "For some classifiers like DecisionTrees, they work directly on ordinal/cat data. For others (NN or SVM), ordinal values mean more importance to some features than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "print('Raw labels are: ', [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "print('The oridnal of [\"tokyo\", \"tokyo\", \"paris\", \"amsterdam\"] is: ', le.transform([\"tokyo\", \"tokyo\", \"paris\", \"amsterdam\"]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM or NN, it means that tokyo is twice weighted as paris!\n",
    "\n",
    "So we better encode cat/oridinal features as OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We cannot use as before, since LabelEncoder and Binarizer work on 1-D array of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_raw = [[1, 2, 2, 6], [1, 2, 2, 6], [1, 2, 2, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(X_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoder works on labels only: 1-D. To make it work on matrix we must convert into pandas data frame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_raw = [[1, 2, 2, 6, 1], [1, 2, 2, 6, 1], [1, 2, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_ordinal = X.apply(le.fit_transform)\n",
    "\n",
    "import numpy as np\n",
    "#print('The oridnal values are: \\n', np.array(X_ordinal))\n",
    "print('The oridnal values are: \\n', X_ordinal)\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform(X_ordinal))\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "X_OHE = LabelBinarizer().fit_transform(X_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? We see all features are assigned 0!!\n",
    "\n",
    "This is because, pandas dataframe is created in colomns. The LabelEncoder scans each colomn independently!\n",
    "So in each col we can see the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see more appealing example (each feature has variations in colomns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "print(X)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "#X_ordinal = X.apply(le.fit_transform)\n",
    "le.fit_transform(X)\n",
    "X_oridnal = le.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above because we cannot fit a transform on X.\n",
    "\n",
    "Instead we can only fit it on COLOMNS of X.\n",
    "\n",
    "To do this, we use X.apply, which works per colomn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "print(X)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_ordinal = X.apply(le.fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "#print('The oridnal values are: \\n', np.array(X_ordinal))\n",
    "print('The oridnal values are (pandas DF): \\n', X_ordinal)\n",
    "#print('The oridnal values are: \\n', X_ordinal.toarray())\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform(X_ordinal))\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "X_OHE = LabelBinarizer().fit_transform(X_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above means, since we do not fit a UNIVERSAL transform for X, but only per colomn, then we cannot apply inverse_transform globally.\n",
    "\n",
    "What we need to do is to have a LabelEncoder transform object PER COLOMN/FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le_dict = {}\n",
    "for col in X.columns:\n",
    "    #print(col.name)\n",
    "    #print(X[col].name)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    le_dict[X[col].name] = le\n",
    "    \n",
    "print('After transform: \\n', X)\n",
    "for col in X.columns:\n",
    "    \n",
    "    X[col] = le_dict[X[col].name].inverse_transform(X[col])\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done with shorthand lambda expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X)\n",
    "\n",
    "X = X.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform categorial features into ordinal ones, and inverse that.\n",
    "\n",
    "Now, we want to fit a OHE.\n",
    "\n",
    "Let's try with LabelBinarizer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X_OHE = LabelBinarizer().fit_transform(X_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, LabelBinarize like LabelEncoder work only on 1-D labels array.\n",
    "\n",
    "Let's try to work on the colomns data of X_ordinal as we did with LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "X_OHE = X_ordinal.apply(lb.fit_transform)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb_dict = {}\n",
    "X_OHE = pd.DataFrame()\n",
    "for col in X_ordinal.columns:\n",
    "    lb = LabelBinarizer()\n",
    "    a = lb.fit_transform(X_ordinal[col])\n",
    "    X_OHE[col] = a\n",
    "    lb_dict[X_ordinal[col].name] = lb\n",
    "    \n",
    "\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)\n",
    "\n",
    "for col in X_OHE.columns:\n",
    "    \n",
    "    X_ordinal_orig[col] = lb_dict[X_OHE[col].name].inverse_transform(X_OHE[col])\n",
    "print('After inverse transform: \\n', X_ordinal_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above is because we are trying to put 3 values (one hot encoding of 0--> 1,0,0) for each colomn.\n",
    "\n",
    "### sklearn provides a ready make class for OHE on features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE_ = enc.fit_transform(X_ordinal) \n",
    "X_OHE = X_OHE_.toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't even need to use LabelEncoder first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data are:  [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
      "(3, 11)\n",
      "The one-hot-encoding: \n",
      " [[ 1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  1.  1.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  1.  1.  0.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE_ = enc.fit_transform(X_raw) \n",
    "X_OHE = X_OHE_.toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape is still 3 rows, but 11 colomns, since we 3 values for first and second feature/colomn, and only 2 (1 and 6) for the 3rd one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse is not possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'inverse_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-baf5bdbda004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_orig_ordinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_OHE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'inverse_transform'"
     ]
    }
   ],
   "source": [
    "X_orig_ordinal = enc.inverse_transform(X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding on categorial (names) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data are:  [['A', 'D', 'G', 'A', 'K'], ['B', 'E', 'H', 'C', 'L'], ['C', 'F', 'J', 'B', 'M']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e7c9175c8293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_OHE_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_OHE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_OHE_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_OHE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2017\u001b[0m         \"\"\"\n\u001b[1;32m   2018\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[0;32m-> 2019\u001b[0;31m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[1;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, selected, copy)\u001b[0m\n\u001b[1;32m   1807\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \"\"\"\n\u001b[0;32m-> 1809\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                       force_all_finite)\n\u001b[1;32m    401\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'A'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE_ = enc.fit_transform(X_raw) \n",
    "X_OHE = X_OHE_.toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work without LabelEnbcoder first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE = enc.fit_transform(X_ordinal).toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we may use DictVictorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data are:  [['A', 'D', 'G', 'A', 'K'], ['B', 'E', 'H', 'C', 'L'], ['C', 'F', 'J', 'B', 'M']]\n",
      "(3, 15)\n",
      "The one-hot-encoding: \n",
      " [[ 1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.]]\n",
      "[{'0=A': 1.0, '1=D': 1.0, '3=A': 1.0, '2=G': 1.0, '4=K': 1.0}, {'4=L': 1.0, '0=B': 1.0, '1=E': 1.0, '2=H': 1.0, '3=C': 1.0}, {'4=M': 1.0, '0=C': 1.0, '3=B': 1.0, '1=F': 1.0, '2=J': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "\n",
    "X = pd.DataFrame(X_raw).to_dict( orient = 'records' )\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "enc = DictVectorizer()\n",
    "X_OHE = enc.fit_transform(X).toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n",
    "print(enc.inverse_transform(X_OHE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why it generates OHE directly?\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer\n",
    "    \n",
    "However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by OneHotEncoder to complete binary one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.  1.]\n",
      " [ 0.  1.  3.]]\n",
      "The one-hot-encoding: \n",
      " [[ 0.  1.  1.  0.  1.  0.]\n",
      " [ 1.  0.  0.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE = enc.fit_transform(X).toarray()\n",
    "print('The one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data: the main API implemented by scikit-learn is that of the estimator. \n",
    "\n",
    "An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data.\n",
    "\n",
    "All estimator objects expose a fit method that takes a dataset (usually a 2-d array) (n_samples, n_features):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator parameters: \n",
    "All the parameters of an estimator can be set when it is instantiated or by modifying the corresponding attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "http://scikit-learn.org/stable/tutorial/basic/tutorial.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
