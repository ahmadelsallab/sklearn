{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample datasets:\n",
    "    scikit-learn comes with a few standard datasets, for instance the iris and digits datasets for classification and the boston house prices dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset contains:\n",
    "    - data: [n_samples, n_features]\n",
    "    - target: [n_samples,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digits.data)\n",
    "print(digits.data.shape)#(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You could plot from data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.reshape(digits.data[0], [8, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(digits.target)\n",
    "print(digits.target.shape)\n",
    "print(digits.images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = iris\n",
    "#print(dataset.data)\n",
    "print(dataset.data.shape)#(n_samples, n_features)\n",
    "#print(dataset.target)\n",
    "print(dataset.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = boston\n",
    "#print(dataset.data)\n",
    "print(dataset.data.shape)#(n_samples, n_features)\n",
    "#print(dataset.target)\n",
    "print(dataset.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load and prepare data\n",
    "- Model selection\n",
    "- Model fitting\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model = SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In sklearn, we have:\n",
    "    - SVC --> SVM Classifier\n",
    "    - SVR --> SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit ao all data except the last example--> keep for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(digits.data[:-1], digits.target[:-1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(digits.data[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(digits.data[-1:], [8, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model persistence (save/load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using python only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "s = pickle.dumps(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_loaded = pickle.loads(s)\n",
    "print(model_loaded.predict(digits.data[-1:]))\n",
    "plt.imshow(np.reshape(digits.data[-1:], [8, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pickle has some security and maintainability issues. Please refer to section Model persistence (http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence) for more detailed information about model persistence with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the specific case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump & joblib.load), which is more efficient on big data, but can only pickle to the disk and not to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'model_joblib.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_loaded_joblib = joblib.load('model_joblib.pkl') \n",
    "print(model_loaded_joblib.predict(digits.data[-1:]))\n",
    "plt.imshow(np.reshape(digits.data[-1:], [8, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn estimators follow certain rules to make their behavior more predictive.\n",
    "Check: http://scikit-learn.org/stable/tutorial/basic/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unless otherwise specified, input will be cast to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import random_projection\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.rand(10, 2000)\n",
    "X = np.array(X, dtype='float32')\n",
    "print('Input type is: ', X.dtype)\n",
    "\n",
    "\n",
    "transformer = random_projection.GaussianRandomProjection()\n",
    "X_new = transformer.fit_transform(X)\n",
    "print('After sklearn fit_transform, the input is cast to float64', X_new.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above rule is applicable only to regression targets, not classification. Classification targets are mentained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "iris = datasets.load_iris()\n",
    "clf = SVC()\n",
    "clf.fit(iris.data, iris.target)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list(clf.predict(iris.data[:3]))\n",
    "\n",
    "\n",
    "clf.fit(iris.data, iris.target_names[iris.target])  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list(clf.predict(iris.data[:3]))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-fitting hyper params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters of an estimator can be updated after it has been constructed via the sklearn.pipeline.Pipeline.set_params method. Calling fit() more than once will overwrite what was learned by any previous fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.rand(100, 10)\n",
    "y = rng.binomial(1, 0.5, 100)\n",
    "X_test = rng.rand(5, 10)\n",
    "\n",
    "\n",
    "clf = SVC()\n",
    "print('Initial SVC model constructed')\n",
    "\n",
    "print('Override the kernel to linear')\n",
    "clf.set_params(kernel='linear').fit(X, y)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(clf.predict(X_test))\n",
    "\n",
    "\n",
    "print('Override the kernel to rbf')\n",
    "clf.set_params(kernel='rbf').fit(X, y)  \n",
    "\n",
    "\n",
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target labels encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between ordinal and nominal\n",
    "http://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/\n",
    "\n",
    "    \n",
    "## Nominal: \n",
    "\n",
    "Nominal scales are used for labeling variables, without any quantitative value.  “Nominal” scales could simply be called “labels.”  Here are some examples, below.  Notice that all of these scales are mutually exclusive (no overlap) and none of them have any numerical significance.  A good way to remember all of this is that “nominal” sounds a lot like “name” and nominal scales are kind of like “names” or labels.\n",
    "    \n",
    "    \n",
    "## Oridnal: \n",
    "\n",
    "With ordinal scales, it is the order of the values is what’s important and significant, but the differences between each one is not really known.  Take a look at the example below.  \n",
    "\n",
    "In each case, we know that a #4 is better than a #3 or #2, but we don’t know–and cannot quantify–how much better it is.  \n",
    "\n",
    "For example, is the difference between “OK” and “Unhappy” the same as the difference between “Very Happy” and “Happy?”  We can’t say.\n",
    "\n",
    "Ordinal scales are typically measures of non-numeric concepts like satisfaction, happiness, discomfort, etc.\n",
    "\n",
    "“Ordinal” is easy to remember because is sounds like “order” and that’s the key to remember with “ordinal scales”–it is the order that matters, but that’s all you really get from these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorial case (LableEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input is an array of strings, and we want to fit NOMINAL or ORDINAL (higher value is higher importance), we use LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "print('Raw labels are: ', [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "print('The oridnal of [\"tokyo\", \"tokyo\", \"paris\"] is: ', le.transform([\"tokyo\", \"tokyo\", \"paris\"]) )\n",
    "\n",
    "print('The inverse transofrm into strings of [2 2 1] is: ', list(le.inverse_transform([2, 2, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may even have input categories as numbers, not as strings. \n",
    "\n",
    "We may want to give them ascending order \"RANKs\" (0,1,2,3...etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Raw labels are: ', [1, 2, 2, 6])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([1, 2, 2, 6])\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "print('The oridnal of [1, 1, 2, 6] is: ', le.transform([1, 1, 2, 6]) )\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform([0, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass case on ordinal data (LabelBinarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classifiers are binary, only produce 1 or 0. To transform it into multi-class case (n_classes), a common way is to use one-vs-all.\n",
    "\n",
    "In this case, you n_classes classifiers, each is trained on 0/1 task. If the input instance belongs to a class, then its desired output is 1 and all others are zeros.\n",
    "\n",
    "This mandates to encode the targets in a special way, also known as one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "print('Normally, the target is formatted as: ', y)\n",
    "\n",
    "y = LabelBinarizer().fit_transform(y)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The one-hot-encoding is mandatory for the NN softmax for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encoder (OHE) on categorial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we transform to ordinal using LabelEncoder then use LabelBinarizer to get the OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_raw = [1, 2, 2, 6]\n",
    "print('Raw labels are: ', y_raw)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_raw)\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "y_ordinal = le.transform(y_raw)\n",
    "print('The oridnal of [1, 1, 2, 6] is: ', y_ordinal )\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform(y_ordinal))\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "y_OHE = LabelBinarizer().fit_transform(y_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', y_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-labels vs. Multi-Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we may need to assign more than one label for an instance/example.\n",
    "\n",
    "This is encountered in case of multi-task learning.\n",
    "\n",
    "Example: https://arxiv.org/abs/1612.07695\n",
    "        \n",
    "In this case, we can encode more than one position as 1 at a time.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]\n",
    "y = MultiLabelBinarizer().fit_transform(y)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One vs ALL classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After formatting the target variables, how to fit a one vs. ALL classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "\n",
    "y = LabelBinarizer().fit_transform(y)\n",
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneVsRestClassifier can also work on categorial y directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "\n",
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Working with Categorial data in the input features (not targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset (Design matrix X [n_samples, n_features].\n",
    "\n",
    "And we have ALL or SOME of the features as categorial/ordinal.\n",
    "\n",
    "For some classifiers like DecisionTrees, they work directly on ordinal/cat data. For others (NN or SVM), ordinal values mean more importance to some features than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "print('Raw labels are: ', [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "print('The available UNIQUE classes are: ', list(le.classes_))\n",
    "\n",
    "print('The oridnal of [\"tokyo\", \"tokyo\", \"paris\", \"amsterdam\"] is: ', le.transform([\"tokyo\", \"tokyo\", \"paris\", \"amsterdam\"]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM or NN, it means that tokyo is twice weighted as paris!\n",
    "\n",
    "So we better encode cat/oridinal features as OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We cannot use as before, since LabelEncoder and Binarizer work on 1-D array of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_raw = [[1, 2, 2, 6], [1, 2, 2, 6], [1, 2, 2, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(X_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoder works on labels only: 1-D. To make it work on matrix we must convert into pandas data frame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_raw = [[1, 2, 2, 6, 1], [1, 2, 2, 6, 1], [1, 2, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_ordinal = X.apply(le.fit_transform)\n",
    "\n",
    "import numpy as np\n",
    "#print('The oridnal values are: \\n', np.array(X_ordinal))\n",
    "print('The oridnal values are: \\n', X_ordinal)\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform(X_ordinal))\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "X_OHE = LabelBinarizer().fit_transform(X_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? We see all features are assigned 0!!\n",
    "\n",
    "This is because, pandas dataframe is created in colomns. The LabelEncoder scans each colomn independently!\n",
    "So in each col we can see the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see more appealing example (each feature has variations in colomns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "print(X)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "#X_ordinal = X.apply(le.fit_transform)\n",
    "le.fit_transform(X)\n",
    "X_oridnal = le.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above because we cannot fit a transform on X.\n",
    "\n",
    "Instead we can only fit it on COLOMNS of X.\n",
    "\n",
    "To do this, we use X.apply, which works per colomn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "print(X)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_ordinal = X.apply(le.fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "#print('The oridnal values are: \\n', np.array(X_ordinal))\n",
    "print('The oridnal values are (pandas DF): \\n', X_ordinal)\n",
    "#print('The oridnal values are: \\n', X_ordinal.toarray())\n",
    "\n",
    "print('The inverse transofrm into strings of [0, 0, 1, 2] is: ', le.inverse_transform(X_ordinal))\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "X_OHE = LabelBinarizer().fit_transform(X_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above means, since we do not fit a UNIVERSAL transform for X, but only per colomn, then we cannot apply inverse_transform globally.\n",
    "\n",
    "What we need to do is to have a LabelEncoder transform object PER COLOMN/FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le_dict = {}\n",
    "for col in X.columns:\n",
    "    #print(col.name)\n",
    "    #print(X[col].name)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    le_dict[X[col].name] = le\n",
    "    \n",
    "print('After transform: \\n', X)\n",
    "for col in X.columns:\n",
    "    \n",
    "    X[col] = le_dict[X[col].name].inverse_transform(X[col])\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done with shorthand lambda expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X)\n",
    "\n",
    "X = X.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform categorial features into ordinal ones, and inverse that.\n",
    "\n",
    "Now, we want to fit a OHE.\n",
    "\n",
    "Let's try with LabelBinarizer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X_OHE = LabelBinarizer().fit_transform(X_ordinal)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, LabelBinarize like LabelEncoder work only on 1-D labels array.\n",
    "\n",
    "Let's try to work on the colomns data of X_ordinal as we did with LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "\n",
    "\n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "X_OHE = X_ordinal.apply(lb.fit_transform)\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb_dict = {}\n",
    "X_OHE = pd.DataFrame()\n",
    "for col in X_ordinal.columns:\n",
    "    lb = LabelBinarizer()\n",
    "    a = lb.fit_transform(X_ordinal[col])\n",
    "    X_OHE[col] = a\n",
    "    lb_dict[X_ordinal[col].name] = lb\n",
    "    \n",
    "\n",
    "print('We can use the LabelBinarizer to perform the one-hot-encoding: \\n', X_OHE)\n",
    "\n",
    "for col in X_OHE.columns:\n",
    "    \n",
    "    X_ordinal_orig[col] = lb_dict[X_OHE[col].name].inverse_transform(X_OHE[col])\n",
    "print('After inverse transform: \\n', X_ordinal_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above is because we are trying to put 3 values (one hot encoding of 0--> 1,0,0) for each colomn.\n",
    "\n",
    "### sklearn provides a ready make class for OHE on features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE_ = enc.fit_transform(X_ordinal) \n",
    "X_OHE = X_OHE_.toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't even need to use LabelEncoder first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[1, 2, 2, 6, 1], [2, 8, 2, 2, 1], [4, 10, 2, 6, 6]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE_ = enc.fit_transform(X_raw) \n",
    "X_OHE = X_OHE_.toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape is still 3 rows, but 11 colomns, since we 3 values for first and second feature/colomn, and only 2 (1 and 6) for the 3rd one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse is not possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_orig_ordinal = enc.inverse_transform(X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding on categorial (names) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE_ = enc.fit_transform(X_raw) \n",
    "X_OHE = X_OHE_.toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work without LabelEnbcoder first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "    \n",
    "print('After transform: \\n', X_ordinal)\n",
    "\n",
    "X_orig = X_ordinal.apply(lambda x: le_dict[x.name].inverse_transform(x))\n",
    "    \n",
    "print('After inverse transform: \\n', X_orig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE = enc.fit_transform(X_ordinal).toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we may use DictVictorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "\n",
    "X = pd.DataFrame(X_raw).to_dict( orient = 'records' )\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "enc = DictVectorizer()\n",
    "X_OHE = enc.fit_transform(X).toarray()\n",
    "print(X_OHE.shape)\n",
    "print('The one-hot-encoding: \\n', X_OHE)\n",
    "\n",
    "print(enc.inverse_transform(X_OHE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why it generates OHE directly?\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer\n",
    "    \n",
    "However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by OneHotEncoder to complete binary one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X_OHE = enc.fit_transform(X).toarray()\n",
    "print('The one-hot-encoding: \\n', X_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding categorial data\n",
    "The word embedding means to transform/embed an input representation into another space representation.\n",
    "\n",
    "In case of categorial variables, you can think of it as indexing a lookup table with the nominal values of the features, each indexed entry is a vector representing the embedding of this nominal integer in the new space.\n",
    "\n",
    "The table size shall be: [max_feature_nominal_value, embedding_vector_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data are:  [['A', 'D', 'G', 'A', 'K'], ['B', 'E', 'H', 'C', 'L'], ['C', 'F', 'J', 'B', 'M']]\n",
      "Before transform: \n",
      "    0  1  2  3  4\n",
      "0  A  D  G  A  K\n",
      "1  B  E  H  C  L\n",
      "2  C  F  J  B  M\n",
      "[[0 0 0 0 0]\n",
      " [1 1 1 2 1]\n",
      " [2 2 2 1 2]]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_raw = [[\"A\", \"D\", \"G\", \"A\", \"K\"], [\"B\", \"E\", \"H\", \"C\", \"L\"], [\"C\", \"F\", \"J\", \"B\", \"M\"]]\n",
    "print('Raw data are: ', X_raw)\n",
    "\n",
    "X = pd.DataFrame(X_raw)\n",
    "\n",
    "print('Before transform: \\n', X)\n",
    "\n",
    "from collections import defaultdict\n",
    "le_dict = defaultdict(preprocessing.LabelEncoder)\n",
    "\n",
    "# X.apply loops on colomns, so small x is the col\n",
    "X_ordinal = X.apply(lambda x: le_dict[x.name].fit_transform(x))\n",
    "X = np.array(X_ordinal)\n",
    "print(X)\n",
    "print(X[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model with Embedding\n",
    "Embedding is a ML concept. The LUT as described above can be though of as a weight matrix  [max_feature_nominal_value, embedding_vector_size], where the weights are learnable. They are initialized as randoms, and learnt as optimizer progresses.\n",
    "\n",
    "In Keras this can be achieved easily with Embedding layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "(?, 1, 50)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_201 (Embedding)    (None, 1, 50)             150       \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,807\n",
      "Trainable params: 6,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, GRU, Embedding, concatenate, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "#params\n",
    "dr_r = 0.1\n",
    "\n",
    "#Inputs\n",
    "input_l = Input(shape=[1])\n",
    "\n",
    "print(input_l.shape)\n",
    "\n",
    "\n",
    "#Embeddings layers\n",
    "max_feature_nominal_value = 3\n",
    "embedding_vector_size = 50\n",
    "emb_category = Embedding(max_feature_nominal_value, embedding_vector_size, input_length=1)(input_l)\n",
    "print(emb_category.shape)\n",
    "# Embedding layer generates [batch_size, seq_lenght=input_length=1 in pur case, embedding_size=50 in out case]\n",
    "# So we have (?,1,50)--> generating output ?,1,1 not ?,1 for regression task. S owe need to flatten the ?,1,50 of embedding layer usiing Flatten layer\n",
    "\n",
    "main_l = Dropout(dr_r) (Dense(128) (Flatten()(emb_category)))\n",
    "\n",
    "\n",
    "#output\n",
    "output = Dense(1, activation=\"linear\") (main_l)\n",
    "\n",
    "#model\n",
    "model = Model(input_l, output)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 4.1639      \n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 4.0338     \n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 3.8890     \n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 3.7313     \n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 3.6839      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e7d8b8ec88>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.random.randn(len(X),)\n",
    "print(y.shape)\n",
    "X_ = X[:,0]\n",
    "print(X_.shape)# Note that, here we have 3, --> so 2dims, it's ok for Input([1]), but the 3 will be considered the num_batches\n",
    "#FITTING THE MODEL\n",
    "BATCH_SIZE = 1\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X[:,0], y, epochs=epochs, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: we have 5 categorial features above, not one (each col is a feature, of max nominal value = 2, so 3 values (0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<keras.engine.topology.InputLayer object at 0x000001E7D619DBA8>, 0, 0)\n",
      "(?, 5, 1)\n",
      "(<keras.layers.core.Lambda object at 0x000001E7D617A2B0>, 0, 0)\n",
      "(?, 1)\n",
      "(?, 1)\n",
      "(?, 1, 50)\n",
      "(?, ?)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_60 (InputLayer)            (None, 5, 1)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)               (None, 1)             0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)               (None, 1)             0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)               (None, 1)             0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)               (None, 1)             0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)               (None, 1)             0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_196 (Embedding)        (None, 1, 50)         150         lambda_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "embedding_197 (Embedding)        (None, 1, 50)         150         lambda_34[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "embedding_198 (Embedding)        (None, 1, 50)         150         lambda_35[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "embedding_199 (Embedding)        (None, 1, 50)         150         lambda_36[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "embedding_200 (Embedding)        (None, 1, 50)         150         lambda_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 50)            0           embedding_196[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 50)            0           embedding_197[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 50)            0           embedding_198[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 50)            0           embedding_199[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 50)            0           embedding_200[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)     (None, 250)           0           flatten_8[0][0]                  \n",
      "                                                                   flatten_9[0][0]                  \n",
      "                                                                   flatten_10[0][0]                 \n",
      "                                                                   flatten_11[0][0]                 \n",
      "                                                                   flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_55 (Dense)                 (None, 128)           32128       concatenate_31[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (None, 128)           0           dense_55[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_56 (Dense)                 (None, 1)             129         dropout_29[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 33,007\n",
      "Trainable params: 33,007\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, GRU, Embedding, concatenate, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "#params\n",
    "dr_r = 0.1\n",
    "\n",
    "#Inputs\n",
    "input_l = Input(shape=[5,1])\n",
    "print(input_l._keras_history)\n",
    "print(input_l.shape)\n",
    "\n",
    "#category0 = Input(shape=[1])\n",
    "#category0 = input_l[:,0]# --> This is invalid since category0 is not an output of keras layer anymore, so it doesnt have any of the keras layer attribs like _leras_history\n",
    "#print(category0.shape) #--> (?,) while it should be (?,1) so that Embedding layer can be indexed\n",
    "#category0 = tf.contrib.keras.backend.expand_dims(input_l[:,0])\n",
    "category0 = Lambda(lambda x: x[:,0])(input_l)\n",
    "print(category0._keras_history)\n",
    "print(category0.shape)\n",
    "category1 = Lambda(lambda x: x[:,1])(input_l)\n",
    "print(category1.shape)\n",
    "category2 = Lambda(lambda x: x[:,2])(input_l)\n",
    "category3 = Lambda(lambda x: x[:,3])(input_l)\n",
    "category4 = Lambda(lambda x: x[:,4])(input_l)\n",
    "\n",
    "#Embeddings layers\n",
    "max_feature_nominal_value = 3\n",
    "embedding_vector_size = 50\n",
    "emb_category0 = Embedding(max_feature_nominal_value, embedding_vector_size)(category0)\n",
    "print(emb_category0.shape)\n",
    "emb_category1 = Embedding(max_feature_nominal_value, embedding_vector_size)(category1)\n",
    "emb_category2 = Embedding(max_feature_nominal_value, embedding_vector_size)(category2)\n",
    "emb_category3 = Embedding(max_feature_nominal_value, embedding_vector_size)(category3)\n",
    "emb_category4 = Embedding(max_feature_nominal_value, embedding_vector_size)(category4)\n",
    "\n",
    "#main layer\n",
    "main_l = concatenate([\n",
    "   Flatten()(emb_category0) # Without flatten we add extra 1,50 dim due to input 5,1\n",
    "    , Flatten()(emb_category1)\n",
    "    , Flatten()(emb_category2)\n",
    "    , Flatten()(emb_category3)\n",
    "    , Flatten()(emb_category4)\n",
    "])\n",
    "print(main_l.shape)\n",
    "\n",
    "main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "\n",
    "\n",
    "#output\n",
    "output = Dense(1, activation=\"linear\") (main_l)\n",
    "\n",
    "#model\n",
    "model = Model(input_l, output)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 5)\n",
      "(3, 5, 1)\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s - loss: 0.0057     \n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s - loss: 0.0077         \n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s - loss: 0.0013     \n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s - loss: 0.0013         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e7d7510d68>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randn(len(X),)\n",
    "print(y.shape)\n",
    "print(X.shape) # (3,5) missing (3,5,1) since Input is (5,1) measn ?,5,1 when we add batch size\n",
    "#X = np.reshape(X, [X.shape[0], X.shape[1], 1])\n",
    "X = np.expand_dims(X,axis=-1)\n",
    "print(X.shape)\n",
    "#FITTING THE MODEL\n",
    "BATCH_SIZE = 1\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data: the main API implemented by scikit-learn is that of the estimator. \n",
    "\n",
    "An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data.\n",
    "\n",
    "All estimator objects expose a fit method that takes a dataset (usually a 2-d array) (n_samples, n_features):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator parameters: \n",
    "All the parameters of an estimator can be set when it is instantiated or by modifying the corresponding attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Every estimator exposes a score method that can judge the quality of the fit (or the prediction) on new data.\n",
    " \n",
    " Each estimator provides a special score function.\n",
    " \n",
    " For example, SVC (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html):\n",
    " \n",
    " score(X, y[, sample_weight])\tReturns the mean accuracy on the given test data and labels.\n",
    " \n",
    " The score method is normally the accuracy of prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100])\n",
    "svc.score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy might not be the best measure (for unbalanced classes data). A dataset of 1000 examples of class A and 10 examples of class B will have 99% accuacy by only predicting any given example as coming from class A:\n",
    "    \n",
    "    Accuracy = 1000/(1000 + 10) = 0.99\n",
    "    \n",
    "In this case you might want to score different metrics like precision, recall of F1 score.\n",
    "\n",
    "You can find many different metrics functions under sklearn.metrics: http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y_digits[-100:]\n",
    "y_pred = svc.predict(X_digits[-100:])\n",
    "precision_recall_fscore_support(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note precision, recall and F1 are defined for binary classes. In case of multi-classes you have to take an average. Different settings of averages exist: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html:\n",
    "\n",
    "\n",
    "average : string, [None (default), ‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’]\n",
    "\n",
    "If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "'binary':\n",
    "\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "'micro':\n",
    "\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "'macro':\n",
    "\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "'weighted':\n",
    "\n",
    "Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "'samples':\n",
    "\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score function works on test set. Sometimes the total data you have is small; so holding aside a test set will reduce the training data, and the test size will not be statistically significant.\n",
    "\n",
    "In this case, it is recommended to split the dataset into different cuts (K folds), each cut will split the data in say K-1 folds train and 1 fold test, which is now called development or dev set. This will create K different datasets with different K splits. \n",
    "\n",
    "The best practice is to average the evaluation score over the K splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"]\n",
    "K = 3\n",
    "k_fold = KFold(n_splits=K)\n",
    "print(k_fold)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply it to SVC example of digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train, test in k_fold.split(X_digits):\n",
    "    print(svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints the score for the 3 folds, which we can average.\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a helper function for the above for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_jobs=-1 means that the computation will be dispatched on all the CPUs of the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods for CV splits. Check under (Cross-validation generators, http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model to use?\n",
    "\n",
    "Note that: to this, we have to fit 3 models to 3 different datasets.\n",
    "\n",
    "We cannot use any of them. Keep in mind the CV methods are either for evaluation, or for hyper params tuning. \n",
    "\n",
    "For hyper params tuning case, once we decide on the best model, we train based on ALL the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation and hyper params tuning\n",
    "\n",
    "The KFolds can also serve for hyper-parameters tuning.\n",
    "\n",
    "In general the test set MUST remain unseen during the whole training process. \n",
    "\n",
    "We cannot tune any parameter OR hyper parameter according to our performance on the test set. Otherwise, the model might memorize or ovefit the test set.\n",
    "\n",
    "However, we have many hyper params values to choose from. \n",
    "\n",
    "For each setting, we learn from train set, then we want to evaluate on unseen data, which cannot be the test data. So we have a third dataset called dev set.\n",
    "\n",
    "Now we can formulate hte hyper params tunining as a search problem in different settings:\n",
    "\n",
    "For each setting, we learn from train set, and evaulate on dev set. Then we may change the value of the hyper params according to how well we did on the dev set and re-train and evaluate again and so on.\n",
    "\n",
    "If the dataset is small, we may use cross-valiation as a means to have more representative train and dev sets for hyper params tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search and cross-validated estimators¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])        \n",
    "\n",
    "clf.best_score_                                  \n",
    "\n",
    "clf.best_estimator_.C                            \n",
    "\n",
    "\n",
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default is K=3 folds. You can override this by passing a cv object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "\n",
    "K = 10\n",
    "k_fold = KFold(n_splits=K)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(estimator=svc, cv=k_fold, param_grid=dict(C=Cs), n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])        \n",
    "\n",
    "clf.best_score_                                  \n",
    "\n",
    "clf.best_estimator_.C                            \n",
    "\n",
    "\n",
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to cross validate the model after grid search is performed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "k_fold = KFold(n_splits=K)\n",
    "cross_val_score(clf, X_digits, y_digits, cv=k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining in sklear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that some estimators can transform data and that some estimators can predict variables. \n",
    "\n",
    "You may need to pre-process, then fit a classifier. This whole pipeline is you estimator, that you want to pass for GridSearch for example.\n",
    "\n",
    "\n",
    "We can also create combined estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_digits)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n",
    "\n",
    "# Prediction\n",
    "n_components = [20, 40, 64]\n",
    "Cs = np.logspace(-4, 4, 3)\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              logistic__C=Cs))\n",
    "estimator.fit(X_digits, y_digits)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full example on face recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================================================\n",
    "Faces recognition example using eigenfaces and SVMs\n",
    "===================================================\n",
    "\n",
    "The dataset used in this example is a preprocessed excerpt of the\n",
    "\"Labeled Faces in the Wild\", aka LFW_:\n",
    "\n",
    "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
    "\n",
    ".. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Expected results for the top 5 most represented people in the dataset:\n",
    "\n",
    "================== ============ ======= ========== =======\n",
    "                   precision    recall  f1-score   support\n",
    "================== ============ ======= ========== =======\n",
    "     Ariel Sharon       0.67      0.92      0.77        13\n",
    "     Colin Powell       0.75      0.78      0.76        60\n",
    "  Donald Rumsfeld       0.78      0.67      0.72        27\n",
    "    George W Bush       0.86      0.86      0.86       146\n",
    "Gerhard Schroeder       0.76      0.76      0.76        25\n",
    "      Hugo Chavez       0.67      0.67      0.67        15\n",
    "       Tony Blair       0.81      0.69      0.75        36\n",
    "\n",
    "      avg / total       0.80      0.80      0.80       322\n",
    "================== ============ ======= ========== =======\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "http://scikit-learn.org/stable/tutorial/basic/tutorial.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
